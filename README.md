# WhatsApp AI Agent

An end-to-end WhatsApp concierge that blends rule-based automation with LangChain tools and a local LLM. The project ships with a FastAPI webhook, a WhatsApp Web bridge, background reminder scheduler, and persistent user memory so you can build rich appointment workflows.

## Highlights

- **Full messaging pipeline.** Incoming WhatsApp messages are collected by a small Node.js bridge and forwarded to the FastAPI webhook for intent handling and AI fallbacks.
- **Structured LangChain agent.** The agent blends local Ollama responses with keyword fallbacks so development and tests work even without a running model server.
- **Booking + reminders out of the box.** Opinionated handlers, LangChain tools, and an APScheduler job manage bookings, reschedules, cancellations, and multi-channel reminders.
- **Memory microservice.** A lightweight MCP-compatible FastAPI service persists user state for the agent and other components.
- **Docker or manual dev flows.** Container recipes are provided for both the FastAPI service and the WhatsApp bridge, and there are Python helpers for local development.

## Architecture at a glance

```
WhatsApp ↔ whatsapp-bot (Node.js bridge) → FastAPI webhook → routing & tools → LangChain agent
                                                              ↘ reminder scheduler (APS)
                                                              ↘ memory MCP service (FastAPI)
```

1. The WhatsApp bridge (`whatsapp-bot/index.js`) maintains a web session with WhatsApp Web, exposes `/send`, and forwards messages to the FastAPI webhook.
2. `src/receiver.py` parses the payload, routes to booking helpers, reminder/email flows, or invokes the LangChain agent for more complex requests.
3. Tools in `src/tools/` encapsulate calendar access, WhatsApp sending, and simple utilities that the agent can call as structured actions.
4. Scheduled reminders run every minute to send 24-hour and 1-hour alerts via WhatsApp and optional email.
5. User memory lives in `src/memory/mcp_server.py` and is accessed through `src/memory/memory_client.py` so the agent can stay context-aware across messages.

## Quick start

### Prerequisites

- Python 3.11+ (the Dockerfile uses 3.11) and Node.js 18+ for the bridge.
- A working WhatsApp account to scan the QR code generated by `whatsapp-web.js`.
- (Optional) An Ollama instance with the `qwen2.5` model if you want full LLM responses. The agent degrades gracefully to keyword heuristics otherwise.

### 1. Clone and install Python dependencies

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Install Node dependencies for the bridge

```bash
cd whatsapp-bot
npm install
cd ..
```

### 3. Configure environment variables

Create a `.env` file in the project root and add any credentials you need:

```env
# WhatsApp bridge
FASTAPI_PORT=8001

# Optional email reminders
SMTP_HOST=smtp.example.com
SMTP_PORT=587
SMTP_USER=bot@example.com
SMTP_PASS=super-secret
EMAIL_FROM=studio@example.com
```

Environment variables are read by both the Python and Node services when present.

### 4. Launch the stack

Pick the workflow that best fits your setup:

- **Local Python processes** (helpful for development):
  ```bash
  # Terminal 1 – FastAPI webhook + reminder scheduler
  python start_backend.py

  # Terminal 2 – WhatsApp bridge (scan the QR code once prompted)
  cd whatsapp-bot
  npm start
  ```

- **Docker Compose** (runs both services together):
  ```bash
  docker compose up --build
  ```

- **All-in-one helper** (Windows specific spawner that also starts Duckling and the MCP memory server):
  ```bash
  python launch_all.py
  ```

Once running, send a WhatsApp message to the linked number and watch the logs for routing decisions and tool usage.

### 5. Run automated checks

Unit tests focus on the evaluation harness and are optional when `mlflow` is not installed. Install `mlflow` if you would like to execute them instead of skipping:

```bash
pytest
```

The test suite writes MLflow runs to `mlruns/`, which is now ignored from version control; a `.gitkeep` placeholder is committed so the directory exists without history.

## Repository layout

```
.
├── src/
│   ├── receiver.py              # FastAPI webhook and routing logic
│   ├── agent/                   # LangChain agent, run_agent entry-point
│   ├── tools/                   # LangChain tool wrappers (booking, WhatsApp, availability, time)
│   ├── handlers/                # Rule-based booking/reminder handlers
│   ├── reminder_scheduler.py    # APScheduler job for WhatsApp/email reminders
│   └── memory/                  # Memory MCP FastAPI app and REST client
├── whatsapp-bot/                # Node.js WhatsApp bridge (express + whatsapp-web.js)
├── tests/                       # Pytest suite for the evaluation harness
├── Dockerfile                   # FastAPI container definition
├── whatsapp-bot/Dockerfile      # WhatsApp bridge container definition
└── docker-compose.yml           # Compose file to run both services together
```

## LangChain tools available

| Tool | Description |
| --- | --- |
| `SendWhatsappMsg` | Sends templated WhatsApp messages via the local bridge and catches delivery errors so the webhook never crashes. |
| `BookingTool` | Lists available slots, parses numeric selections or natural language, and records the booking atomically. |
| `CheckAvailabilityTool` | Verifies whether a slot is free and suggests the next opening when possible. |
| `CheckBookingTool` | Returns the caller's current appointment if one exists. |
| `GetTime` | Reports the current timestamp for quick replies. |

The agent automatically appends the `@c.us` suffix when missing and persists context such as last messages, email addresses, and booking summaries in the memory service.

## Operational tips

- **Stateful data** lives under `data/` (bookings and memory) and in `mlruns/`. Both directories are ignored by Git; remove them if you need a clean slate.
- **Reminder delivery** uses the WhatsApp bridge and optional SMTP credentials. Missing SMTP settings simply log a warning and skip email sends.
- **LLM connectivity** is optional. When the local Ollama endpoint is unreachable or returns malformed JSON, the agent falls back to deterministic keyword routing so development and CI do not depend on external services.

## Contributing

1. Fork the repository and create a Python virtual environment.
2. Run `pytest` before opening a pull request.
3. Document new tools or handlers in this README and keep runtime artefacts out of version control.

## License

This project is released under the MIT License. See [LICENSE](LICENSE).
